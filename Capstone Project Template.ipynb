{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "\n",
    "# __v pyreadstat for reading multiple files sas7dat\n",
    "# import os\n",
    "# import pyreadstat as pyd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "\n",
    "# To test read Immigration data file by pandas\n",
    "# v_ read the immigration data here\n",
    "# [!Note]: should care when large files, this just try read a file\n",
    "\n",
    "#_________V_________V_________V_________V_________V_________V_________V_________V\n",
    "#Bypass this for save time run test, use only pyspark later on\n",
    "\n",
    "\n",
    "#  _v: read only one file i94_apr16_sub\n",
    "# fname_immigra = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "# df_AprImmig = pd.read_sas(fname_immigra, 'sas7bdat', encoding=\"ISO-8859-1\")\n",
    "\n",
    "# to print out the data frame of Immigration data\n",
    "# df_AprImmig.head(7)\n",
    "\n",
    "#_________V_________V_________V_________V_________V_________V_________V_________V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v_ read temperature data here\n",
    "# read by pandas\n",
    "\n",
    "#_________V_________V_________V_________V_________V_________V_________V_________V\n",
    "#  work OK but, by pass read by pandas here, use only pyspark below\n",
    "#  because, for check the NaN value need to be used the: pyspark.sql.function\n",
    "\n",
    "# fname_temper = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "# df_Temper = pd.read_csv(fname_temper)\n",
    "\n",
    "# # to print out the temprature df\n",
    "# df_Temper.head(10)\n",
    "#_________V_________V_________V_________V_________V_________V_________V_________V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  test read only one sas7bdat file i94_apr16_sub\n",
    "# read by df in pyspark \n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "enableHiveSupport().getOrCreate()\n",
    "\n",
    "#  read only one file into df_spark\n",
    "\n",
    "# df_spark = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n",
    "# print(\"______v______v______v______v to see df when read one file\")   \n",
    "# df_spark.count()\n",
    "\n",
    "#  do read temperature data here\n",
    "\n",
    "fname_temper = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "# df_Temper = pd.read_csv(fname_temper)\n",
    "df_Temper = spark.read.csv(fname_temper)\n",
    "print(\"to see df_Temper.count: {}\".format(df_Temper.count()))\n",
    "\n",
    "# write parquet for df_Temper\n",
    "df_Temper.write.parquet(\"Temp_data_0\")\n",
    "df_Temper=spark.read.parquet(\"Temp_data_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for read multiple files sas\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "# to get the list of full path of each file in Immigration data folder\n",
    "Immig_datapath= '../../data/18-83510-I94-Data-2016'\n",
    "namefiles = os.listdir(Immig_datapath)\n",
    "\n",
    "# return list of full path files\n",
    "files = [os.path.join(Immig_datapath, f) for f in namefiles if os.path.isfile(os.path.join(Immig_datapath, f))]\n",
    "\n",
    "print(\"to see list of path files\")\n",
    "print(files) \n",
    "\n",
    "\n",
    "def unionAll(*dfs):\n",
    "    return reduce(DataFrame.unionAll, dfs)\n",
    "i = 0\n",
    "\n",
    "# accumulating the df for get Immigration data in all 12 files\n",
    "print(\"______v______v______v______v______v______v______v______v \\n\")\n",
    "print(\"To see accumulating df.count for 12 data files .... \\n\")\n",
    "for file in files:\n",
    "#tempo to test first 3 file\n",
    "#     if(i==3): \n",
    "#         break\n",
    "    if(i==0):\n",
    "        df_ImmigAll = spark.read.format('com.github.saurfang.sas.spark').load(file)\n",
    "        cols= df_ImmigAll.columns\n",
    "        print(df_ImmigAll.count())\n",
    "    if(i>0):\n",
    "        #df1=spark.read.format('com.github.saurfang.sas.spark').load(file)\n",
    "        df_ImmigAll = unionAll(df_ImmigAll,spark.read.format('com.github.saurfang.sas.spark').load(file).select([col for col in cols]))\n",
    "        print(df_ImmigAll.count())\n",
    "    i = i+1\n",
    "\n",
    "print(\"______v______v______v______v______v______v______v______v \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write to parquet when read one dat file\n",
    "# df_spark.write.parquet(\"sas_data_viet\")\n",
    "# df_spark=spark.read.parquet(\"sas_data_viet\")\n",
    "\n",
    "# write to parquet when read all 12 files\n",
    "df_ImmigAll.write.parquet(\"sas_data_all\")\n",
    "df_ImmigAll=spark.read.parquet(\"sas_data_all\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing cleaning tasks here\n",
    "# Identify task to explore data and cleaning steps\n",
    "\n",
    "# Task1: Handling with NaN value (missing value)\n",
    "#  -> Indentify where NaN come from (which column on each df)\n",
    "#  -> count the Nan value\n",
    "#  -> select method to handle the NaN value (might be depend on the Nan come from which field of df)\n",
    "\n",
    "# [airport_code]:\n",
    "#     -> the column `name`: does not have the NaN value\n",
    "#     -> but, there are two duplicated name\n",
    "\n",
    "# [cites_demography]:\n",
    "#     -> Some columns does not have NaN: City/ State/ State Code/ Race/ \n",
    "\n",
    "# [Immigration dataL: i94_xxx16_sub.sas7bdat]\n",
    "# need to be indentified the NaN by df method\n",
    "\n",
    "# [Temerature data: GlobalLandTemperaturesByCity.csv]\n",
    "#   -> column 'City' and 'Country' does not have NaN value\n",
    "\n",
    "# Task2: Duplicate data:\n",
    "#  -> how identify>\n",
    "#  -> how to handle with duplicating data\n",
    "\n",
    "# Task3: Some un-expected value come into field of df (Ex. text-string come into a ditgit-column)\n",
    "#  -> shall be removed\n",
    "#  -> replace by a default proper value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,isnan,when,count\n",
    "\n",
    "# Task1: Handling with NaN value (missing value)\n",
    "# TODO Identify which table, which column contain the NaN value\n",
    "\n",
    "# _________v_________v_________v_________v_________v_________v_________v\n",
    "#  use the read csv by pandas\n",
    "#  Check NaN in df of Temerature\n",
    "\n",
    "# print(\"to see NaN value in interest columns ... \\n\")\n",
    "# NaN_dt=df_Temper['dt'].isna().sum()\n",
    "# NaN_AverageTemperature=df_Temper['AverageTemperature'].isna().sum()\n",
    "# NaN_City=df_Temper['City'].isna().sum()\n",
    "# NaN_Country=df_Temper['Country'].isna().sum()\n",
    "# print(\"NaN value number of column dt is: {} \\n\".format(NaN_dt))\n",
    "# print(\"NaN value number of column AverageTemperature is: {} \\n\".format(NaN_AverageTemperature))\n",
    "# print(\"NaN value number of column City is: {} \\n\".format(NaN_City))\n",
    "# print(\"NaN value number of column Country is: {} \\n\".format(NaN_Country))\n",
    "# print(\"\\n\")\n",
    "# _________v_________v_________v_________v_________v_________v_________v\n",
    "print(\"just list out the Temper df \\n {}\".format(list(df_ImmigAll)))\n",
    "\n",
    "print(\"to check type of df_Temper is {}\".format(type(df_Temper)))\n",
    "\n",
    "print(\"to see all Columns of Terperature Df with counting NaN ... \\n\")\n",
    "df_Temper.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_Temper.columns]).show()\n",
    "\n",
    "#  Check NaN in df of Immigration data\n",
    "# Get a list of all columns in immigration df\n",
    "# Immig_col_list = list(df_ImmigAll)\n",
    "# print(\"To see all the columns list of df_ImmigAll is \\n {}\".format(Immig_col_list))\n",
    "\n",
    "\n",
    "print(\"to see all NaN value of df_ImmigAll \\n\")\n",
    "print(type(df_ImmigAll))\n",
    "# df_ImmigAll.isnull().sum(axis = 0)\n",
    "\n",
    "\n",
    "# here to print out all Columns of Immigration DataFrame with counting NaN\n",
    "# df_ImmigAll.select([count(when(isnan(c), c)).alias(c) for c in df_ImmigAll.columns]).show()\n",
    "df_ImmigAll.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_ImmigAll.columns]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task2: Duplicate data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task3: Some un-expected value come into field of df (Ex. text-string come into a ditgit-column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform quality checks here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "66b7616eaaf41318df583b15f75d9cf6a7bf392840ba2a497becbfa49442bc5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
