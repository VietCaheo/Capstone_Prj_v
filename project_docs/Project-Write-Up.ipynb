{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "Project to acquiring data from provided-workspace and extract information about: Immigration to US; Cities in US ; Airport in US.\n",
    "Project intend to make a target tables for serving info-query in future (such as Security Investigate for people who immigrated to or leaved out US), by doing proper ETL job base on above datasets.\n",
    "Workspace supplied also the world-Temprature data, this will be optional-work for attach more climate information or coordinates by cities.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets using for project: use supplied data by Udacity workspace.\n",
    "\n",
    "##### Immigration Datasets:\n",
    "* Comes from 12 file with format sas7bdat files.\n",
    "* This data supply main information about immigration activities to US, that was acquired in all of months 2016.\n",
    "* dataset will be used as main source to do ETL job for get target table for monitor who immigrated and leaved out US in 2016.\n",
    "\n",
    "##### Airport dataset:\n",
    "* The dataset provide information for airports on the world with some detail info such as : Airport Name, country, municipality (could understand as city in this project), coordinate of each one\n",
    "* From municipality and US-State code suppplied in it, they should come to make joining data later.\n",
    "\n",
    "##### US-cities demographic:\n",
    "* The file provide detail information of each US cities: some information should support joining data such as State Code and City.\n",
    "\n",
    "##### GlobalLandTemperatureByCity:\n",
    "* This data come with more than 1 millions rows consists detail average temperature by city, expand on very long range of record time.\n",
    "* This data will using when do join with AirPortCode and CityName, that can make a useful table for query climate information and location also for each city in specified time.\n",
    "\n",
    "##### [NOTE-1] The step for read and acquiring data from above datasets have done in etl.py\n",
    "##### [NOTE-2] The evidence for project includes datasets requires (at least 2 data sources; more than 1 millions line of data; at leaset two data sources/ format).\n",
    "* The number of rows of Temperature data is >8 million lines\n",
    "* The number of rows of Immigration data is >3 million lines per one file (totally is 12 immigration data files)\n",
    "* Above information can be found in console result when test run etl.py\n",
    "* In `./project_docs/dataset_evidence.pdf` has contains the evidence as mentions above\n",
    "* In `./project_docs/dataset_evidence.pdf` also contains the SQL demo query result for `to group count immigrants by their cities and include the \"foreign_born\" field in this city`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "##### By Doing read datasets mentioned in Step1 either pandas read df or PySpark read Df, could be explore more about each dataset.\n",
    "(attached pdf file: `./project_docs/data_explore.pdf`).\n",
    "\n",
    "##### Immigration Datasets: \n",
    "(Please note: these observe below is for demo by one sas data file, for save time run test ETL; Whole datasets always can be read and acquired, as long as cleaningData function have subsetList for user use for droping NaN or Duplicated value during any time run ETL)\n",
    "* By using the api to check NaN for each schema, found some point:\n",
    "* Some important column of this data sets does not include any NaN value: cicid; arrdate; i94port; i94visa; or visatype\n",
    "* The columns does not contain NaN values are important to consider choose vale for query data, or making Join with another dataset.\n",
    "\n",
    "##### US-Cities Demogrphic:\n",
    "* This dataset is the best in Nan consisting, it fews columns have NaN , with maximum Nan values is 16. Almost no need to care these NaN value in this project.\n",
    "* For Duplicated value: as immagine use-case for Security investigation base on Immigration AirPort base on CityName, so the duplicated `City` will be consider to dropping during data cleaning.\n",
    "         However, need to be considered number of cityname would be dropped.\n",
    "\n",
    "##### Airport-Code:\n",
    "* This dataset is quite good quality, important columns such as: ident, type, name, iso_region and coordinates do not contain any NaN value. This will be very good for checking location of airport in future. Another columns contain NaN ,it depend on user want to drop or not.\n",
    "\n",
    "##### Temperature dataset:\n",
    "* This dataset contains a lot missing value, especcially Average Temperature data.\n",
    "* Duplicated value: Just handling during data cleaning step.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Steps necessary to clean the data on each dataset:\n",
    "\n",
    "##### Immigration Datasets: \n",
    "* In this poject:use the i94port for extract AirPortCode base on CityName (use by the sas_labels_descriptionfile), this airport code and cityname may not exist all in another dataset. However in Immigration Datasets, i94port not contain any NaN: that mean any case immigration or leave Us can be tracked by AirPort, (and there a number US cityname can be found from that)\n",
    "* Some columns cotain a lot of NaN such as: gender/visapost/ entdepd. However, user can consider the target table using purpose to make drop the NaN on that table or not.\n",
    "* For Duplicated values: not impact a lot during ETL, just add subset while drop such as (`cicid`): just for make sure not duplicated the Immigration information for same one person on same datetime.\n",
    "* Another note: `arrdate` and `depdate` are provided as `timestamp` type, during loading data to table, it need to transform the ts to iso format type for Date and Time.\n",
    "     \n",
    "##### US-Cities Demogrphic:\n",
    "* No need to care these NaN value in this project.\n",
    "* For Duplicated value: as immagine use-case for Security investigation base on Immigration AirPort base on CityName, so the duplicated `City` will be consider to dropping during data cleaning. However, need to be considered number of cityname would be dropped.\n",
    "    \n",
    "##### Airport-Code:\n",
    "* Columns contain NaN ,it depend on user want to drop or not, in this project not care about that NaN.\n",
    "* Duplicated value: should care about `ident` and `name`: just for make sure in target table do not have duplicated same airport during checking.\n",
    "    \n",
    "##### Temperature dataset:\n",
    "* They will be droped NaN value of AvgTemperatures during data cleaning. Because in same city, have plenty of temperature data was recorded.\n",
    "* Duplicated value: do drop the `city`: just for keeping only one city contain temperature data, another important info as Coordinates is always be same for a city.\n",
    "    \n",
    "##### Note: \n",
    "* Cleaning job: is located as dedicated py module `schemas_assist.DataClean`\n",
    "* In the file DataClean do data cleaning for all dataset type:\n",
    " * for each data set, data cleaning function could offer either subset list for NaN or for duplicated or both of them.\n",
    " * another extra-job in this file is: extract Airport Code + City Name from `SAS_Labels_Description file`. This combined information will work as a key for two dimension table Us-Citiies/ WorldTemp and AirPort; and all of them can be join to Immigration data set for query work later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Project choose the data model Star Schema, reason to choose this:\n",
    "* Dataset need to handle is quite large (each file sasdata more than 3mils rows, Temp > 1mils rows).\n",
    "* And the dataset might be scale up in future, as well as the table need to be scale al so\n",
    "* Need to leverage some tool for speed up staging and writing for big amount data size: Ex.using Pyspark DataFrame.\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "Steps necessary to pipeline the data into the chosen data model:\n",
    "* Using PySpark read data set with some different file format: csv, sas7bdat.\n",
    "* Each time finish read a file, do read on Schema with api printSchema()\n",
    "* Loding data from above staging Schema in to the target table (for Ex. Dimensional Airport table).\n",
    "* Veriry the content inside table just created above\n",
    "* Write table to parquet file:\n",
    "   * consider choosing properly parameter for partitionBy() : for speed up writing and reading parquet files\n",
    "   * choosing location to save parquet file: in real world, it should choosed AWS S3 bucket. For run test of this project, it still use local saving.\n",
    "\n",
    "#### Conceptual Data Model:\n",
    "The StarSchema was descripe in Table.xlsx file in `./project_docs/Table.xlsx`, please kindly find out in that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model.\n",
    " - Implemented in etl.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks: it locate in etl.py also"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file.\n",
    "\n",
    "\n",
    "Data dictionary will be included by Table.xlsx file in the `./project_docs/Table.xlsx`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "##### Why choice PySpark for handling datasets\n",
    "* Data sets provied with large number of rows (millions of rows in two datasets)\n",
    "* To handling, Join Data ... PySpark obviously better choice than pandas df.\n",
    "* We can leverage the operation as dealing SQL command when need to Join, Select, Aggre with df by Spark\n",
    "* Data will be scale up more and more in future , especially Imiggration Data. We need a tools, technologies for support huge computing, storaging resource. In real world, it's absoluty need to Cloud services (Ex. AWS: S3, Redshift, EC2)\n",
    "* When do ETL with Spark: with Staging and Loading task, that we can choose either local or Cloud servie (like S3, Redshift, EC2).\n",
    "\n",
    "##### Propose how often the data should be updated and why.\n",
    "* The main data set for serving immigration information checking is Immigration Data. This is really big amount size.\n",
    "* The second big dataset is City Temperature Data in whole world, this will scale up very fast due to time interval to get climate data.\n",
    "    * For this type of data, prior to update Temeperature data to table, it should be care for cleaning data.\n",
    "* For keep update information: interval time should be 1 week to month that depend on require.\n",
    "* Reason:\n",
    "    * For keep information up to date, it should be often update new info added, however, with this size of datasets, it will consume tons of resource and time.\n",
    "    * So for high require information update: it should be update weekle, with another with lower demand: it can be update with interval 2weeks to 1 month.\n",
    "    \n",
    "##### Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    "     * If data come more really big, it totally need to use Cloud Computing service: we have two choice AWS service for working with Spark: AWS EMR or AWS EC2:\n",
    "         * Both of them could be able to suppport Distributed computing and support HDFS\n",
    "         * AWS EMR use S3 protocol faster than S3a which applied on AWS EC2\n",
    "         * Basically we can choose either of them, with notice AWS EMR will be higer cost a bit.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.:\n",
    "     * It suggest to use AirFlow.\n",
    "     * AirFlow with strongest to make schedule for data pipeline execution\n",
    " * The database needed to be accessed by 100+ people.\n",
    "     * Still come to AirFlow and AWS, with high demand of access data from user, we can:\n",
    "     * Accomplish data from some where locate in S3, load for analythic like Redshift , and loading back database storage location.\n",
    "     * All the ETL with big size amount should done by Cloud computing resource. AirFlow will monitor the flow and schedule for ETL jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
